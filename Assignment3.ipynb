{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "The purpose of this assignment is to get you more familiar with scikit-learn. Using the same principles from **Lab 3a** and **Lab 3b**, you'll be building a classifier that, given a list of tumor biopsy features, will determine whether a breast tumor is malignant or benign!\n",
    "\n",
    "The dataset is from the Breast Cancer Wisconsin Diagnostic Database.  It is a classic in that has been used in many machine learning and statistics courses.  \n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preparing the data\n",
    "\n",
    "To train a machine learning model, we first need data. The dataset we'll be using is located in `data/breastcancer_data.csv`\n",
    "\n",
    "Here is what each column of the file represents, as well as its domain.\n",
    "\n",
    "            Attribute                 Domain\n",
    "    1. Sample code number            id number\n",
    "    2. Clump Thickness               1 - 10\n",
    "    3. Uniformity of Cell Size       1 - 10\n",
    "    4. Uniformity of Cell Shape      1 - 10\n",
    "    5. Marginal Adhesion             1 - 10\n",
    "    6. Single Epithelial Cell Size   1 - 10\n",
    "    7. Bare Nuclei                   1 - 10\n",
    "    8. Bland Chromatin               1 - 10\n",
    "    9. Normal Nucleoli               1 - 10\n",
    "    10. Mitoses                       1 - 10\n",
    "    11. Class:                        (2 for benign, 4 for malignant)\n",
    "    \n",
    "   \n",
    "Unless you have a background in biology, you may not be familiar with what each of these attributes means. As data scientists, however, you're able to recognize that some attributes will be more useful than others in training a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Task 1**: Load the data\n",
    "Read in the breast cancer data from the CSV file and create a dataframe using `pandas.\n",
    "`. Print the first five rows.  Produce a summary of the data.\n",
    "\n",
    "**NOTE:** The csv file we provide has no header. Notice that if you use the [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function from Pandas without specifying the `names` parameter, it takes the first row of the data as the column names. This is not what we want!\n",
    "\n",
    "Instead, pass in a list of column names in your call to `read_csv`. Finally, store the result in a variable named `df`.\n",
    "\n",
    "Hint: Use the following [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) methods:\n",
    "* To print out the first n (5, by default) rows, use 'DataFrame.head(n=5)'. \n",
    "* Use 'DataFrame.describe()' to produce a nice data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sample_code_number  clump_thickness  cell_size_uniformity  \\\n",
      "count        6.830000e+02       683.000000            683.000000   \n",
      "mean         1.076720e+06         4.442167              3.150805   \n",
      "std          6.206440e+05         2.820761              3.065145   \n",
      "min          6.337500e+04         1.000000              1.000000   \n",
      "25%          8.776170e+05         2.000000              1.000000   \n",
      "50%          1.171795e+06         4.000000              1.000000   \n",
      "75%          1.238705e+06         6.000000              5.000000   \n",
      "max          1.345435e+07        10.000000             10.000000   \n",
      "\n",
      "       uniformity of cell shape  marginal_adhesion  \\\n",
      "count                683.000000         683.000000   \n",
      "mean                   3.215227           2.830161   \n",
      "std                    2.988581           2.864562   \n",
      "min                    1.000000           1.000000   \n",
      "25%                    1.000000           1.000000   \n",
      "50%                    1.000000           1.000000   \n",
      "75%                    5.000000           4.000000   \n",
      "max                   10.000000          10.000000   \n",
      "\n",
      "       single_epithelial_cell_size  bare_nuclei  bland_chromatin  \\\n",
      "count                   683.000000   683.000000       683.000000   \n",
      "mean                      3.234261     3.544656         3.445095   \n",
      "std                       2.223085     3.643857         2.449697   \n",
      "min                       1.000000     1.000000         1.000000   \n",
      "25%                       2.000000     1.000000         2.000000   \n",
      "50%                       2.000000     1.000000         3.000000   \n",
      "75%                       4.000000     6.000000         5.000000   \n",
      "max                      10.000000    10.000000        10.000000   \n",
      "\n",
      "       normal nucleoli     mitoses       class  \n",
      "count       683.000000  683.000000  683.000000  \n",
      "mean          2.869693    1.603221    2.699854  \n",
      "std           3.052666    1.732674    0.954592  \n",
      "min           1.000000    1.000000    2.000000  \n",
      "25%           1.000000    1.000000    2.000000  \n",
      "50%           1.000000    1.000000    2.000000  \n",
      "75%           4.000000    1.000000    4.000000  \n",
      "max          10.000000   10.000000    4.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_code_number</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>cell_size_uniformity</th>\n",
       "      <th>uniformity of cell shape</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>single_epithelial_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_code_number  clump_thickness  cell_size_uniformity  \\\n",
       "0             1000025                5                     1   \n",
       "1             1002945                5                     4   \n",
       "2             1015425                3                     1   \n",
       "3             1016277                6                     8   \n",
       "4             1017023                4                     1   \n",
       "\n",
       "   uniformity of cell shape  marginal_adhesion  single_epithelial_cell_size  \\\n",
       "0                         1                  1                            2   \n",
       "1                         4                  5                            7   \n",
       "2                         1                  1                            2   \n",
       "3                         8                  1                            3   \n",
       "4                         1                  3                            2   \n",
       "\n",
       "   bare_nuclei  bland_chromatin  normal nucleoli  mitoses  class  \n",
       "0            1                3                1        1      2  \n",
       "1           10                3                2        1      2  \n",
       "2            2                3                1        1      2  \n",
       "3            4                3                7        1      2  \n",
       "4            1                3                1        1      2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['sample_code_number', 'clump_thickness', 'cell_size_uniformity', 'uniformity of cell shape', \\\n",
    "         'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin',        \\\n",
    "         'normal nucleoli', 'mitoses', 'class']\n",
    "df = pd.read_csv('~/Desktop/data1030/a3-sjmccorm1993/data/breastcancer_data.csv', names=names) \n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Task 2:** Using lab3a and lab3b as a reference, answer the following questions\n",
    "\n",
    "1. Which attributes in your data will you include in the model? In other words, which attributes above will be your **features**?\n",
    "\n",
    "2. Which attributes will you not include in the model? Why?\n",
    "\n",
    "3. What is the **target**/**response**, the attribute we are trying to predict? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**PUT YOUR ANSWER HERE**\n",
    "\n",
    "1) I will use all available attributes of each sample that might have an effect on the class of the biopsy (benign vs. malignant). The features will be:\n",
    "\n",
    "clump_thickness, cell_size_uniformity, uniformity of cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland_chromatin, normal nucleoli, mitoses\n",
    "\n",
    "2) Sample code number will not be included in the model, since this contains no information on the biopsy except for how it was indexed.\n",
    "\n",
    "3) The class variable is the response we're trying to predict, which represents whether the tumor is malignant or benign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use scikit-learn to train a model based on our data, we need the data to be in the format scikit-learn requests.\n",
    "\n",
    "From lab3a, recall the following requirements for working with data in scikit-learn:\n",
    "\n",
    "#### Requirements for working with data in scikit-learn\n",
    "\n",
    "1. Features and response are **separate objects**\n",
    "2. Features and response should be **numeric**\n",
    "3. Features and response should be **NumPy arrays**\n",
    "4. Features and response should have **specific shapes**\n",
    "\n",
    "\n",
    "All of our data is in a single dataframe. We don't satisfy the first requirement, and thus, none of the following requirements.\n",
    "\n",
    "### **Task 3:** Separate your dataframe into two NumPy arrays\n",
    "You will be creating two new NumPy arrays, *data* and *target*. Use your answers to Task 2 to guide you. Are there any columns in the dataframe you can ignore?\n",
    "\n",
    "**Hints:**\n",
    "* The target values are in the `class` column.\n",
    "* Panda's dataframes store tabular data internally using 'numpy' arrays.  You can access this data directly using `Dataframe.values`\n",
    "\n",
    "If you're confused, look at **Lab 3a** as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clump_thickness', 'cell_size_uniformity', 'uniformity of cell shape',\n",
      "       'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei',\n",
      "       'bland_chromatin', 'normal nucleoli', 'mitoses'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# you may consider df.pop('id'), but there are other ways to select columns too!\n",
    "\n",
    "# get every column except class and id, then convert that dataframe into a numpy array\n",
    "features = df.drop('class', axis=1)\n",
    "features.drop('sample_code_number', axis=1, inplace=True)\n",
    "\n",
    "print(features.columns)\n",
    "data = features.as_matrix()\n",
    "\n",
    "# Repeat same process for target\n",
    "target = df['class'].values\n",
    "\n",
    "# Test cases, do not change!\n",
    "assert(683, 9) == data.shape\n",
    "assert(683,) == target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 2: Training machine learning models\n",
    "\n",
    "Now that your data is in the correct format for using scikit-learn, you can use `sklearn`'s built in classification models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we'll be using [Decision Trees](http://scikit-learn.org/stable/modules/tree.html) to perform binary classification.\n",
    "\n",
    "### **Task 4:** Train and test your classifier on training data\n",
    "Train and test your classifier on all of your data and test its accuracy on the same data.\n",
    "\n",
    "For this task, you'll first need to initialize your classifier by creating an instance of it and then calling the `fit` method to train it.\n",
    "\n",
    "\n",
    "Hint: To test the model's accuracy, use [sklearn.metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "Look at **Lab 3a** for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# usually we use X to denote training features, and y to denote training targets\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model using the entire dataset\n",
    "tree.fit(X, y)\n",
    "\n",
    "# find the accuracy score of your model by testing it on the entire dataset, again\n",
    "y_pred = tree.predict(X)\n",
    "accuracy = metrics.accuracy_score(y_pred, y)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have been surprised after printing out your accuracy score in **Task 4**. \n",
    "\n",
    "### **Task 5:** What are the drawbacks to training and testing your classifier on the entire dataset? \n",
    "\n",
    "Please write your response in the context of the breast cancer dataset we're using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training and testing a classifier on the entire dataset causes problems because while it will result in very accurate predictions on the training data (the entire dataset, in this case) this leaves no data for validating and testing that our model works well on new data (other data than what the classifier was trained on). \n",
    "\n",
    "In the context of the dataset we're using, this means that we can create a model that classifies whether a patient has breast cancer with 100% accuracy ONLY for the patients in our dataset; however, there is no guarantee that our classifier will be accurate when presented with data gathered from new patients. This has obvious consequences especially in a medical setting where the output of our model may have a big impact on a patient's well-being.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "Instead of simply training and testing your classifier on the entire dataset, you can perform a technique called **K-fold cross validation**. \n",
    "\n",
    "K-fold cross validation splits your dataset into K chunks. Then, for each chunk, it fits and scores the classifier using all the other chunks as the **training set** and the current chunk as the **test set**. We then treat the average of all the scores as the total score of the model.  This provides a more accurate estimate of how your classifier will perform in the real world (i.e. with new data).\n",
    "\n",
    "![5-fold cross-validation](images/07_cross_validation_diagram.png)\n",
    "\n",
    "### **Task 6**: Use cross-validation on your decision tree classifier.\n",
    "Using [sklearn.model_selection.cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), perform 10-fold cross-validation on your decision tree classifier.\n",
    "\n",
    "Print the average of all the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.945927968851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "print(cross_val_score(tree, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 3: Hyperparameter tuning\n",
    "\n",
    "The focus for this assignment will be in Grid Search, which you will be implementing from scratch.\n",
    "\n",
    "See the link below for more information:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)#Grid_search\n",
    "\n",
    "In machine learning, there are parameters that the data scientist can set before training their model. These parameters are called **hyperparameters**. \n",
    "\n",
    "Decision trees have many such parameters, such as `max_features`, `max_depth`, and `min_samples_leaf`. This makes Decision trees perfect for using procedures such as [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or [randomized search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), which find better hyperparameters by maximizing cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Although Grid Search may sound complicated, it can typically be implemented in just a few lines of code.\n",
    "\n",
    "Consider the pseudocode for Grid Search below, which finds the optimal combination of two parameters: `max_features` and `max_depth`. (Note: these are both features in sklearn's DecisionTreeClassifier).\n",
    "\n",
    "```\n",
    "    maxFeaturesList = list of \"reasonable\" values for max_features\n",
    "    maxDepthList = list of \"reasonable\" values for max_depth\n",
    "    \n",
    "    for max_features in maxFeaturesList:\n",
    "        for max_depth in maxDepthList:\n",
    "            if crossValidation(tree(max_features, max_depth)) is highest yet:\n",
    "                bestMaxFeatures = max_features\n",
    "                bestMaxDepth = max_depth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 7**: Implement Grid Search \n",
    "Improved your classifier by coding a grid search algorithm to find the better hyperparameter values for `max_features`, `max_depth`, and `min_samples_leaf`.\n",
    "\n",
    "You are responsible for creating a \"reasonable\" set of values for each hyperparameter. Read the documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) for some ideas.\n",
    "\n",
    "**DO NOT** use `sklearn.model_selection.GridSearchCV`. You can, however, use `cross_val_score` to evaluate the score at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal tree has the following hyperparameters:\n",
      "Max Features:  7\n",
      "Max Depth:  5\n",
      "Min Samples Leaf:  5\n",
      "Accuracy:  0.959006363191\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hints: \n",
    "Initialize a list of reasonable values for each hyperparemeter\n",
    "* maxFeaturesList = ...\n",
    "* maxDepthList = ...\n",
    "* minSamplesLeafList = ...\n",
    "\n",
    "Then train and test your DecisionTreeClassifier using all possible \n",
    "combinations of the selected parameters. This will require \n",
    "len(maxFeatureList)*len(maxDepthList)*len(minSampleLeafList) \n",
    "train and test steps\n",
    "\n",
    "\n",
    "Print the best set of hyperparemeters and the score obtained \n",
    "using those hyperparameters.\n",
    "\"\"\"\n",
    "\n",
    "# There are 9 features in total, so it makes sense to try all integers in the range 1 to 9\n",
    "maxFeaturesList = [i for i in range(1, 10)]\n",
    "\n",
    "# The depth of the tree is the length of the longest path from a root to a leaf\n",
    "# Assume that this length should not be longer than the number of features\n",
    "maxDepthList = [i for i in range(1, 10)]\n",
    "\n",
    "# Minimum number of samples required to be at a leaf node (a point where tree does not continue to split)\n",
    "# 1 is a logical value to start with, as this will lead to the largest tree possible; try a range of values\n",
    "# from 0 to 20\n",
    "minSamplesLeafList = [i for i in range(1, 20)]\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "# Loop through all possibilities\n",
    "for max_features in maxFeaturesList:\n",
    "    for max_depth in maxDepthList:\n",
    "        for min_samples_leaf in minSamplesLeafList:\n",
    "            \n",
    "            tree = DecisionTreeClassifier(max_features = max_features, max_depth = max_depth, \\\n",
    "                                          min_samples_leaf=min_samples_leaf)\n",
    "            tree.fit(X, y)\n",
    "\n",
    "            # find the accuracy score of your model by testing it on the entire dataset, again\n",
    "            y_pred = tree.predict(X)\n",
    "            \n",
    "            if cross_val_score(tree, X, y).mean() > accuracy:\n",
    "                \n",
    "                accuracy = cross_val_score(tree, X, y).mean()\n",
    "                \n",
    "                bestMaxFeatures = max_features\n",
    "                bestMaxDepth = max_depth\n",
    "                bestMinSamplesLeaf = min_samples_leaf\n",
    "                \n",
    "         \n",
    "            \n",
    "            \n",
    "# Print results\n",
    "print(\"The optimal tree has the following hyperparameters:\")\n",
    "print(\"Max Features: \", bestMaxFeatures)\n",
    "print(\"Max Depth: \", bestMaxDepth)\n",
    "print(\"Min Samples Leaf: \", bestMinSamplesLeaf)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation of grid search finds hyperparemeters that achieve a cross validation score of 95%. In theory, our classifier would almost always classify tumors correctly. But how often would you classify a benign tumor as malignant? And how often would you classify a malignant tumor as benign?\n",
    "\n",
    "### **Task 8**:  Confusion Matrix\n",
    "Using the classifier with optimal hyperparameters, construct a confusion matrix that describes its performance\n",
    "\n",
    "Read documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to see how to build a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "We've provided code that splits the dataset into a training set and a test set. Call `fit` using the training set, and then call `predict` and construct the confusion matrix using the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103   4]\n",
      " [  8  56]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create tree using optimal parameter values\n",
    "tree = DecisionTreeClassifier(max_features = 5, max_depth = 8, \\\n",
    "                              min_samples_leaf=5)\n",
    "# Fit model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict new values using test data\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# print your confusion matrix here\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 9**: Describe your confusion matrix.\n",
    "\n",
    "What does each value in your confusion matrix correspond to?\n",
    "\n",
    "Identify what the following terms mean in regards to the breast cancer dataset:\n",
    "\n",
    "- **True Positives (TP):** \n",
    "- **True Negatives (TN):** \n",
    "- **False Positives (FP):** \n",
    "- **False Negatives (FN):** \n",
    "\n",
    "In the context of diagnosing breast cancer, is it worse to have a false positive or a false negative? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# True positives: The number of people who the model predicted would have cancer who actually had cancer (53)\n",
    "# True negatives: The number of people who the model predicted would not have cancer who did not have cancer (104)\n",
    "# False positives: The number of people who the model predicted had cancer that did not have cancer (3)\n",
    "# False negatives: The number of people who the model predicted would not have cancer but actually did have cancer (9)\n",
    "\n",
    "In this context, it's much worse to have a false negative; while a cancer diagnosis that turns out to be incorrect is certainly not pleasant for the patient, it's much worse to fail to detect cancer in a patient who needs to receive treatment as soon as possible in order to recover. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Mini-project\n",
    "Perform the same set of classification tasks in Tasks 1 through 9 on a either the [Titanic dataset](https://www.kaggle.com/c/titanic) or a dataset of your choosing.  Use your data from a source other than sklearn.  Include a set of next steps (i.e. suggestions) on how you could improve your classifiers performance.\n",
    "\n",
    "Hints:\n",
    "1. Be careful during data preparation to handle missing values appropriately.  You can either fill them in with estimate (say by the mean of the ones that are present), or you can drop rows or columns that contain them.\n",
    "3. You will need to either drop columns that contain categorical data (unordered data) or convert them to numerical form.\n",
    "2. Converting categorical data (unordered category data) to numerical form requires using [One Hot Encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) or some other appropriate technique.  Here is some [discussion](http://pbpython.com/categorical-encoding.html)\n",
    "3. Use the Z-transform functionality in sklean to convert continuous values to z-scores and use them appropriately. Read this general information on [sklearn preprocessing]( http://scikit-learn.org/stable/modules/preprocessing.html) and then use [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load the Titanic data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('~/Desktop/data1030/a3-sjmccorm1993/data/titanic_train.csv')\n",
    "\n",
    "# Examine shape, column names, and first few rows of data\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 2: Answer questions about which variables to use as features vs. target\n",
    "\n",
    "# Q: Which attributes in your data will you include in the model? \n",
    "\n",
    "# A: I will plan to use the following variables as features: \n",
    "#    Pclass, sex, age, # siblings/spouses aboard, # parents/children aboard, fare, and embarkation point\n",
    "\n",
    "# Q: Which attributes will you not include and why?\n",
    "\n",
    "# A: Passenger ID, since it contains no information about the passenger;\n",
    "#    Name, for the same reason as passenger ID\n",
    "#    Ticket, because I don't have enough knowledge to understand how the ticket information might impact survival\n",
    "#    Cabin, since there are many missing values\n",
    "\n",
    "# Q: What is the target/response, the attribute we are trying to predict?\n",
    "\n",
    "# A: Survived, an indicator variable telling us whether the passenger survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked',\n",
      "       'Survived'],\n",
      "      dtype='object')\n",
      "[3 1 2]\n",
      "['male' 'female']\n",
      "[1 0 3 4 2 5 8]\n",
      "[0 1 2 5 3 4 6]\n",
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Separate the data into data and target arrays\n",
    "\n",
    "# Create features dataframe, and include survival variable for now \n",
    "# (will drop rows based on NA values and target vector length has to match features vector length)\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Survived']\n",
    "features = df[feature_cols]\n",
    "\n",
    "print(features.columns)\n",
    "\n",
    "# Look at distinct unique values for non-continuous variables\n",
    "print(features.Pclass.unique())\n",
    "print(features.Sex.unique())\n",
    "print(features.SibSp.unique())\n",
    "print(features.Parch.unique())\n",
    "print(features.Embarked.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "687\n",
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',\n",
      "       'Survived'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing steps\n",
    "\n",
    "# Two null values in Embarked variable - drop these rows\n",
    "print(features.Embarked.isnull().sum())\n",
    "features = features[pd.notnull(features['Embarked'])]\n",
    "\n",
    "# 687 null values in Cabin variable: won't include this variable in model\n",
    "print(features.Cabin.isnull().sum())\n",
    "features.drop('Cabin', axis=1, inplace=True)\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert gender and embarcation variable to numeric categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "features['Embarked'] = label_encoder.fit_transform(features['Embarked']) \n",
    "features['Sex'] = label_encoder.fit_transform(features['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fare and age variables to z-scores and drop missing values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Remove rows that have null values for fare and age variables\n",
    "features = features[pd.notnull(features['Fare'])]\n",
    "features = features[pd.notnull(features['Age'])]\n",
    "\n",
    "# values.reshape used as a result of a DeprecationError\n",
    "x_fare = features['Fare'].values.reshape(-1, 1)\n",
    "x_age = features['Age'].values.reshape(-1, 1)\n",
    "\n",
    "# Use standard scaler to transform continuous varibles\n",
    "standard_scaler = StandardScaler()\n",
    "features['Fare'] = standard_scaler.fit_transform(X = x_fare) \n",
    "features['Age'] = standard_scaler.fit_transform(X = x_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable from features dataframe and remove it\n",
    "target = features['Survived']\n",
    "features.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "# Convert features df to matrix\n",
    "data = features.as_matrix()\n",
    "\n",
    "# Test cases (two variables and ~200 obs containing missing values dropped from original dataset)\n",
    "assert(712, 7) == data.shape\n",
    "assert(712,) == target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98595505618\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Train classifier on training data\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X = training features, y = target\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "# Instantiate the model (using the default parameters)\n",
    "titanic_tree = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model using the entire dataset\n",
    "titanic_tree.fit(X, y)\n",
    "\n",
    "# find the accuracy score of your model by testing it on the entire dataset\n",
    "y_pred = titanic_tree.predict(X)\n",
    "accuracy = metrics.accuracy_score(y_pred, y)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 5\n",
    "\n",
    "# Training and testing a classifier on the entire dataset causes problems because while it will result in \n",
    "# very accurate predictions on the training data (the entire dataset, in this case) this leaves no data \n",
    "# for validating and testing that our model works well on new data \n",
    "# (other data than what the classifier was trained on). \n",
    "\n",
    "# In the context of the dataset we're using, this means that we can create a model that classifies \n",
    "# whether a passenger will survived with very high accuracy ONLY for the passenger in our training dataset; \n",
    "# however, there is no guarantee that our classifier will be accurate when presented when we test our \n",
    "# model on the official Kaggle \"test\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771386653253\n"
     ]
    }
   ],
   "source": [
    "# Task 6: Perform 10-fold cross validation on model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "print(cross_val_score(titanic_tree, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 7: Implement grid search (using built-in GridSearch module from sklearn this time)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the parameter values that should be searched\n",
    "feat_range = list(range(1, 7))\n",
    "depth_range = list(range(1, 25)) \n",
    "min_samples = list(range(1, 10))\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(max_features=feat_range, max_depth=depth_range, min_samples_leaf=min_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'max_features': 3, 'min_samples_leaf': 3}\n",
      "0.823033707865\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,\n",
      "            max_features=3, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# instantiate the grid\n",
    "grid = GridSearchCV(titanic_tree, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# Run the grid search on the data\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print parameters corresponding to best model\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83 21]\n",
      " [23 51]]\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Produce confusion matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create tree using optimal parameter values\n",
    "titanic_tree_opt = DecisionTreeClassifier(max_features = 4, max_depth = 10, \\\n",
    "                              min_samples_leaf=7)\n",
    "# Fit model\n",
    "titanic_tree_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predict new values using test data\n",
    "y_pred = titanic_tree_opt.predict(X_test)\n",
    "\n",
    "# print your confusion matrix here\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Describe your confusion matrix\n",
    "\n",
    "# True positives: The number of people who the model predicted would survive who actually survived (49)\n",
    "# True negatives: The number of people who the model predicted would not survive who died (89)\n",
    "# False positives: The number of people who the model predicted would surive who died (15)\n",
    "# False negatives: The number of people who the model predicted would not survive who did survive (25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
